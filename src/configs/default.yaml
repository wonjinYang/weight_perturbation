# Default hyperparameters for the Weight Perturbation library.
# This file provides baseline values for models, samplers, losses, perturbation,
# pretraining, and other utilities. Users can override these in custom configs.

# General settings
seed: 42  # Random seed for reproducibility
device: 'cpu'  # Default device ('cpu' or 'cuda')
precision: 'fp32'  # Model precision ('fp32', 'fp16', or 'bf16')

# Model architecture parameters
model:
  noise_dim: 2  # Dimension of input noise for Generator
  data_dim: 2  # Dimension of output data
  hidden_dim: 256  # Number of units in hidden layers
  activation: 'LeakyReLU(0.2)'  # Default activation function

# Sampler parameters
samplers:
  real_data:
    means: [[2.0, 0.0], [-2.0, 0.0], [0.0, 2.0], [0.0, -2.0]]  # Default cluster means
    std: 0.4  # Standard deviation for Gaussian clusters
  target_data:
    shift: [1.8, 1.8]  # Default shift for target clusters
    std: 0.4
  evidence_domains:
    num_domains: 3  # Number of evidence domains
    samples_per_domain: 35  # Samples per domain
    random_shift: 3.4  # Radius for circular placement
    std: 0.7  # Standard deviation around centers
  kde_sampler:
    bandwidth: 0.22  # Default bandwidth for KDE
    num_samples: 160  # Default number of samples to generate
    adaptive: false  # Whether to adapt bandwidth based on local variance
  virtual_target_sampler:
    bandwidth: 0.22  # Base bandwidth for virtual target KDE
    num_samples: 600  # Total samples for virtual target
    temperature: 1.0  # Softmax temperature for domain selection

# Loss function parameters
losses:
  wasserstein_distance:
    p: 2  # Power for Wasserstein distance (1 or 2)
    blur: 0.07  # Entropic regularization parameter
  barycentric_ot_map:
    cost_p: 2  # Power for cost function (e.g., squared Euclidean)
    reg: 0.01  # Regularization for softmin
  global_w2_loss:
    n_samples: 1600  # Samples for loss computation
  multi_marginal_ot_loss:
    blur: 0.06  # Entropic blur for Sinkhorn
    entropy_lambda: 0.012  # Coefficient for entropy regularization

# Perturbation parameters for target-given
perturbation_target_given:
  steps: 100  # Number of perturbation steps
  eta_init: 0.045  # Initial learning rate (increased for faster convergence)
  eta_min: 1e-5  # Minimum learning rate
  eta_max: 0.5  # Maximum learning rate
  eta_decay_factor: 0.9  # Learning rate decay when performance degrades
  eta_boost_factor: 1.05  # Learning rate boost when performance improves
  clip_norm: 0.4  # Gradient clipping norm (congestion bound)
  momentum: 0.95  # Momentum factor for updates
  patience: 20  # Patience for early stopping
  rollback_patience: 15  # Patience for rollback mechanism
  improvement_threshold: 1e-4  # Minimum improvement to be considered significant
  eval_batch_size: 600  # Batch size for evaluation

# Perturbation parameters for evidence-based
perturbation_target_not_given:
  epochs: 100  # Number of perturbation epochs
  eta_init: 0.045  # Initial learning rate (increased for faster convergence)
  eta_min: 1e-4  # Minimum learning rate
  eta_max: 0.5  # Maximum learning rate
  eta_decay_factor: 0.9  # Learning rate decay when performance degrades
  eta_boost_factor: 1.05  # Learning rate boost when performance improves
  clip_norm: 0.4  # Gradient clipping norm
  momentum: 0.975  # Momentum factor
  patience: 20  # Patience for early stopping
  rollback_patience: 15  # Patience for rollback mechanism
  lambda_entropy: 0.012  # Entropy regularization coefficient
  lambda_virtual: 0.8  # Entropy regularization coefficient
  lambda_multi: 1.0  # Entropy regularization coefficient
  improvement_threshold: 1e-3  # Minimum improvement to be considered significant
  eval_batch_size: 600  # Batch size for evaluation and virtual sampling
  bandwidth_base: 0.22  # Base bandwidth for virtual target estimation

# Pretraining parameters for WGAN-GP
pretrain:
  epochs: 300  # Number of pretraining epochs
  batch_size: 96  # Batch size for training
  lr: 0.0002  # Learning rate for Adam optimizers
  betas: [0.0, 0.95]  # Beta parameters for Adam
  gp_lambda: 0.5  # Gradient penalty coefficient
  critic_iters: 5  # Critic updates per generator update