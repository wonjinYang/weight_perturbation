# Default hyperparameters for the Weight Perturbation library.
# This file provides baseline values for models, samplers, losses, perturbation,
# pretraining, and other utilities. Users can override these in custom configs.

# General settings
seed: 42  # Random seed for reproducibility
device: 'cpu'  # Default device ('cpu' or 'cuda')
precision: 'fp32'  # Model precision ('fp32', 'fp16', or 'bf16')

# Model architecture parameters
model:
  noise_dim: 2  # Dimension of input noise for Generator
  data_dim: 2  # Dimension of output data
  hidden_dim: 256  # Number of units in hidden layers
  activation: 'LeakyReLU(0.2)'  # Default activation function

# Sampler parameters
samplers:
  real_data:
    means: [[2.0, 0.0], [-2.0, 0.0], [0.0, 2.0], [0.0, -2.0]]  # Default cluster means
    std: 0.4  # Standard deviation for Gaussian clusters
  target_data:
    shift: [1.8, 1.8]  # Default shift for target clusters
    std: 0.4
  evidence_domains:
    num_domains: 3  # Number of evidence domains
    samples_per_domain: 35  # Samples per domain
    random_shift: 3.4  # Radius for circular placement
    std: 0.7  # Standard deviation around centers
  kde_sampler:
    bandwidth: 0.22  # Default bandwidth for KDE
    num_samples: 160  # Default number of samples to generate
    adaptive: false  # Whether to adapt bandwidth based on local variance
  virtual_target_sampler:
    bandwidth: 0.22  # Base bandwidth for virtual target KDE
    num_samples: 600  # Total samples for virtual target
    temperature: 1.0  # Softmax temperature for domain selection

# Loss function parameters
losses:
  wasserstein_distance:
    p: 2  # Power for Wasserstein distance (1 or 2)
    blur: 0.07  # Entropic regularization parameter
  barycentric_ot_map:
    cost_p: 2  # Power for cost function (e.g., squared Euclidean)
    reg: 0.01  # Regularization for softmin
  global_w2_loss:
    n_samples: 1600  # Samples for loss computation
  multi_marginal_ot_loss:
    blur: 0.06  # Entropic blur for Sinkhorn
    entropy_lambda: 0.012  # Coefficient for entropy regularization

# Perturbation parameters for Section 2 (target-given)
perturbation_section2:
  steps: 24  # Number of perturbation steps
  eta_init: 0.017  # Initial learning rate
  clip_norm: 0.12  # Gradient clipping norm (congestion bound)
  momentum: 0.91  # Momentum factor for updates
  patience: 7  # Patience for early stopping on W2 increase
  eval_batch_size: 1600  # Batch size for evaluation

# Perturbation parameters for Section 3 (evidence-based)
perturbation_section3:
  epochs: 100  # Number of perturbation epochs
  eta_init: 0.045  # Initial learning rate
  clip_norm: 0.23  # Gradient clipping norm
  momentum: 0.975  # Momentum factor
  patience: 6  # Patience for early stopping
  lambda_entropy: 0.012  # Entropy regularization coefficient
  eval_batch_size: 600  # Batch size for evaluation and virtual sampling
  bandwidth_base: 0.22  # Base bandwidth for virtual target estimation

# Pretraining parameters for WGAN-GP
pretrain:
  epochs: 300  # Number of pretraining epochs
  batch_size: 64  # Batch size for training
  lr: 0.0002  # Learning rate for Adam optimizers
  betas: [0.0, 0.9]  # Beta parameters for Adam
  gp_lambda: 10.0  # Gradient penalty coefficient
  critic_iters: 5  # Critic updates per generator update
  eval_batch_size: 1000  # Batch size for evaluation during pretraining

# Utility parameters
utils:
  plot:
    title: "Distribution Comparison"  # Default plot title
    save_path: null  # Default save path (null to disable saving)
    show: false  # Whether to display plots
  logging:
    level: 'INFO'  # Logging level ('DEBUG', 'INFO', 'WARNING', 'ERROR')
    file: 'experiment.log'  # Log file name

# Advanced/Experimental parameters
experimental:
  adaptive_bandwidth: true  # Enable adaptive bandwidth in KDE
  entropy_regularization: true  # Enable entropy reg in losses
  max_retries: 3  # Max retries for failed operations (e.g., NaN losses)
  tolerance: 1e-5  # Numerical tolerance for comparisons
